{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "import shapelets as sha\n",
    "import importlib\n",
    "importlib.reload(sha)\n",
    "from operator import itemgetter\n",
    "random.seed(1)\n",
    "lcs=[]\n",
    "classes=[]\n",
    "xs=np.arange(0,30)\n",
    "def noisy(average_value):\n",
    "    return average_value+(math.cos(random.randint(0,360)*(math.pi/180))*0.1*average_value)\n",
    "no_per_class=50\n",
    "for i in range(no_per_class):\n",
    "    ys=[]\n",
    "    peak=np.random.choice(xs[1:-1])\n",
    "    for x in xs:\n",
    "        y=noisy(2)\n",
    "        ys.append(y)\n",
    "    ys[peak]=ys[peak-1]=ys[peak+1]=noisy(5)\n",
    "    lcs.append(np.stack((xs,ys)))\n",
    "    classes.append(\"alpha\")\n",
    "for i in range(no_per_class):\n",
    "    ys=[]\n",
    "    peak=np.random.choice(xs[:-1])\n",
    "    for x in xs:\n",
    "        y=noisy(2)\n",
    "        ys.append(y)\n",
    "    ys[peak]=noisy(10)\n",
    "    lcs.append(np.stack((xs,ys)))\n",
    "    classes.append(\"beta\")\n",
    "for i in range(no_per_class):\n",
    "    ys=[]\n",
    "    peak=np.random.choice(xs[:-3])\n",
    "    for x in xs:\n",
    "        y=noisy(2)\n",
    "        ys.append(y)\n",
    "    ys[peak]=noisy(10)\n",
    "    #if peak<xs[-2]:\n",
    "    ys[peak+2]=ys[peak+3]=noisy(5)\n",
    "    lcs.append(np.stack((xs,ys)))\n",
    "    classes.append(\"gamma\")\n",
    "for i in range(no_per_class):\n",
    "    ys=[]\n",
    "    #peak=np.random.choice(xs[:-3])\n",
    "    for x in xs:\n",
    "        y=noisy(2)\n",
    "        ys.append(y)\n",
    "    #ys[peak]=noisy(10)\n",
    "    #if peak<xs[-2]:\n",
    "    #ys[peak+2]=ys[peak+3]=noisy(5)\n",
    "    lcs.append(np.stack((xs,ys)))\n",
    "    classes.append(\"delta\")\n",
    "ids=[]\n",
    "for i in range(len(classes)):\n",
    "    ids.append(i)\n",
    "ob_state = {}\n",
    "for i, ob in enumerate(classes):\n",
    "    ob_state[i] = ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test, id_train, id_test = train_test_split(lcs, classes, ids, test_size=0.5, random_state=1, stratify=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-45c96945f401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mbest_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_split_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mskip_shapelet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy_pruning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_gain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbelong_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0;31m#x.append((best_split, distances))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/shapelet_timeseries_classification/shapelets.py\u001b[0m in \u001b[0;36mentropy_pruning\u001b[0;34m(best_gain, distances, best_split, belong_class_count, other_class_count, set_entropy)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mcalc_belong\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mcalc_other\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcalc_belong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mdistances_bcs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#best case scenario when all the distances are included\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mdistances_bcs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mmaxdist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistances_bcs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mappend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_tuple\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_deepcopy_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;31m# We're not going to put the tuple in the memo, but it's still important we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;31m# check for it, in case the tuple contains recursive mutable structures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_deepcopy_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;31m# We're not going to put the tuple in the memo, but it's still important we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;31m# check for it, in case the tuple contains recursive mutable structures.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(sha)\n",
    "best_shapelets=[]\n",
    "time_res=1\n",
    "x=[]\n",
    "for n_donor, lc_donor in enumerate(x_train):\n",
    "    #Create lists with classifications of all time-series relative to the donor time series; one that the pool of shapelets is generated from\n",
    "    state_donor = y_train[n_donor]#y_train[n],x_train[n] and id_train[n] all refer to the attributes of the same time series\n",
    "    belong_class=[]\n",
    "    other_class=[]\n",
    "    for n, i in enumerate(id_train):\n",
    "        if y_train[n] == state_donor:\n",
    "            belong_class.append(i)\n",
    "        else:\n",
    "            other_class.append(i)\n",
    "    #calculate the entropy of the entire set, so it can be compared to the split set later\n",
    "    prop_belong = len(belong_class)/(len(belong_class)+len(other_class))\n",
    "    set_entropy = -(prop_belong)*math.log2(prop_belong)-(1-prop_belong)*math.log2(1-prop_belong)\n",
    "    pool=sha.generate_shapelets(lc_donor, 1, len(lc_donor[0]))#generate shapelets from the donor time-series, \n",
    "    #set the initial best value of information gain to 0 (improved by any split) and start testing the shapelets\n",
    "    best_gain=0\n",
    "    for shapelet in pool:\n",
    "        skip_shapelet=False#for entropy pruning\n",
    "        #set the order of distance calculations\n",
    "        #pick an other_class object first and then alternate between belong and other, when one group runs out, append the rest of the other group to the end\n",
    "        order=[]\n",
    "        if len(belong_class)<len(other_class):alternations=len(belong_class);larger_group=other_class\n",
    "        else: alternations=len(other_class); larger_group=belong_class\n",
    "        for i in range(alternations):\n",
    "            order.append(other_class[i])\n",
    "            order.append(belong_class[i])\n",
    "        for i in range(len(larger_group)-alternations):\n",
    "            order.append(larger_group[-(i+1)])\n",
    "        #start distance calculations\n",
    "        distances=[]\n",
    "        for n_lc in order:\n",
    "            if id_train[n_donor] == n_lc:\n",
    "                distance = 0\n",
    "            else:\n",
    "                lc=x_train[np.where(np.array(id_train)==n_lc)[0][0]]\n",
    "                distance=sha.distance_calculation(shapelet, lc, early_abandon=True)\n",
    "            #save the distance value together with the classification and lightcurve id\n",
    "            if n_lc in belong_class:\n",
    "                class_assign=1\n",
    "            else:\n",
    "                class_assign=0\n",
    "            distances.append((n_lc ,distance, class_assign))\n",
    "            #find the optimal split point if there are at least two distances calculated, then use entropy pruning to find if the shapelet still has a change to beat the best one found so far\n",
    "            if len(distances)>1:\n",
    "                best_split=sha.best_split_point(distances, set_entropy)\n",
    "                skip_shapelet=sha.entropy_pruning(best_gain, distances, best_split, len(belong_class), len(other_class), set_entropy)\n",
    "                #x.append((best_split, distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3 0.5 0.75\n",
      "1.0 0.8112781244591328\n",
      "0.048794940695398525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "distances=[[-1,1,0],[-1,10,1],[-1,15,1],[-1,8,1],\n",
    "          [-1,11,0],[-1,10,0],[-1,3,1],[-1,7,1]]\n",
    "prop_belong=(above_belong+below_belong)/len(distances)\n",
    "split_point=10\n",
    "set_entropy = -(prop_belong)*math.log2(prop_belong)-(1-prop_belong)*math.log2(1-prop_belong)\n",
    "#split_point=sha.best_split_point(distances, set_entropy)\n",
    "above=[lc for lc in distances if lc[1]>=split_point]\n",
    "above_belong=sum([lc[2] for lc in above])\n",
    "below=[lc for lc in distances if lc[1]<split_point]\n",
    "below_belong=sum([lc[2] for lc in below])\n",
    "prop_above_belong=above_belong/len(above)\n",
    "prop_below_belong=below_belong/len(below)\n",
    "if prop_above_belong==1. or prop_above_belong==0.:\n",
    "    above_entropy=0\n",
    "else:\n",
    "    above_entropy = -(prop_above_belong)*math.log2(prop_above_belong)-(1-prop_above_belong)*math.log2(1-prop_above_belong)\n",
    "if prop_below_belong==1. or prop_below_belong==0.:\n",
    "    below_entropy =0\n",
    "else:\n",
    "    below_entropy = -(prop_below_belong)*math.log2(prop_below_belong)-(1-prop_below_belong)*math.log2(1-prop_below_belong)\n",
    "info_gain=set_entropy-(len(above)/(len(above)+len(below)))*(above_entropy)-(len(below)/(len(above)+len(below)))*(below_entropy)\n",
    "print(above_belong, below_belong, prop_above_belong, prop_below_belong)\n",
    "print(above_entropy,below_entropy)\n",
    "print(info_gain)\n",
    "best_gain=0.09235938389499487\n",
    "best_split=split_point\n",
    "belong_class_count=sum([lc[2] for lc in distances])\n",
    "other_class_count=len(distances)-belong_class_count\n",
    "\n",
    "\n",
    "calc_belong=sum([lc[2] for lc in distances])\n",
    "calc_other=len(distances)-calc_belong\n",
    "distances_bcs=deepcopy(distances) #best case scenario when all the distances are included\n",
    "distances_bcs.sort(key=itemgetter(1))\n",
    "maxdist=distances_bcs[-1][1]+1\n",
    "for add_belong in range(belong_class_count-calc_belong):\n",
    "    distances_bcs.append((-1,0,1))\n",
    "for add_other in range(other_class_count-calc_other):\n",
    "    distances_bcs.append((-1,maxdist,0))\n",
    "gain_bcs=sha.information_gain(distances_bcs, set_entropy, best_split)\n",
    "if isinstance(gain_bcs, str) == True:\n",
    "    ans= True\n",
    "else:\n",
    "    if gain_bcs<best_gain:\n",
    "        ans= True\n",
    "    else:\n",
    "        ans= False\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 0.5 0.6666666666666666\n",
      "1.0 0.9182958340544896\n",
      "0.015712127384097774\n"
     ]
    }
   ],
   "source": [
    "print(above_belong, below_belong, prop_above_belong, prop_below_belong)\n",
    "print(above_entropy,below_entropy)\n",
    "print(info_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112781244591328"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 5, 'b': 5}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9182925006850887"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_above_belong=0.33333\n",
    "-(prop_above_belong)*math.log2(prop_above_belong)-(1-prop_above_belong)*math.log2(1-prop_above_belong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5 16\n",
      "[[-5, 1, 0], [-5, 1, 0], [-5, 1, 0], [-5, 1, 0], [-5, 2, 0], [-5, 7, 1], [-5, 7, 1], [-5, 10, 1], [-5, 11, 1], [-5, 15, 1], (-1, 0, 1), (-1, 0, 1), (-1, 0, 1), (-1, 0, 1), (-1, 0, 1), (-1, 16, 0), (-1, 16, 0), (-1, 16, 0), (-1, 16, 0), (-1, 16, 0)]\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances=[[-5,1,0],[-5,1,0],[-5,2,0],[-5,1,0],[-5,1,0],\n",
    "          [-5,11,1],[-5,10,1],[-5,15,1],[-5,7,1],[-5,7,1]]\n",
    "best_gain=0.04556599707503506\n",
    "best_split=4.5\n",
    "belong_class_count=10\n",
    "other_class_count=10\n",
    "set_entropy=1\n",
    "\n",
    "def entropy_pruning(best_gain, distances, best_split, belong_class_count, other_class_count, set_entropy):\n",
    "    calc_belong=sum([lc[2] for lc in distances])\n",
    "    calc_other=len(distances)-calc_belong\n",
    "    distances_bcs=deepcopy(distances) #best case scenario when all the distances are included\n",
    "    distances_bcs.sort(key=itemgetter(1))\n",
    "    maxdist=distances_bcs[-1][1]+1\n",
    "    print(calc_belong, calc_other, maxdist)\n",
    "    for add_belong in range(belong_class_count-calc_belong):\n",
    "        distances_bcs.append((-1,0,1))\n",
    "    for add_other in range(other_class_count-calc_other):\n",
    "        distances_bcs.append((-1,maxdist,0))\n",
    "    print(distances_bcs)\n",
    "    gain_bcs=sha.information_gain(distances_bcs, set_entropy, best_split)\n",
    "    print(gain_bcs)\n",
    "    if isinstance(gain_bcs, str) == True:\n",
    "        return True\n",
    "    else:\n",
    "        if gain_bcs<best_gain:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "entropy_pruning(best_gain, distances, best_split, belong_class_count, other_class_count, set_entropy)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
