{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "import shapelets as sha\n",
    "import importlib\n",
    "importlib.reload(sha)\n",
    "from operator import itemgetter\n",
    "random.seed(1)\n",
    "lcs=[]\n",
    "classes=[]\n",
    "xs=np.arange(0,30)\n",
    "def noisy(average_value):\n",
    "    return average_value+(math.cos(random.randint(0,360)*(math.pi/180))*0.1*average_value)\n",
    "no_per_class=50\n",
    "for i in range(no_per_class):\n",
    "    ys=[]\n",
    "    peak=np.random.choice(xs[1:-1])\n",
    "    for x in xs:\n",
    "        y=noisy(2)\n",
    "        ys.append(y)\n",
    "    ys[peak]=ys[peak-1]=ys[peak+1]=noisy(5)\n",
    "    lcs.append(np.stack((xs,ys)))\n",
    "    classes.append(\"alpha\")\n",
    "for i in range(no_per_class):\n",
    "    ys=[]\n",
    "    peak=np.random.choice(xs[:-1])\n",
    "    for x in xs:\n",
    "        y=noisy(2)\n",
    "        ys.append(y)\n",
    "    ys[peak]=noisy(10)\n",
    "    lcs.append(np.stack((xs,ys)))\n",
    "    classes.append(\"beta\")\n",
    "for i in range(no_per_class):\n",
    "    ys=[]\n",
    "    peak=np.random.choice(xs[:-3])\n",
    "    for x in xs:\n",
    "        y=noisy(2)\n",
    "        ys.append(y)\n",
    "    ys[peak]=noisy(10)\n",
    "    #if peak<xs[-2]:\n",
    "    ys[peak+2]=ys[peak+3]=noisy(5)\n",
    "    lcs.append(np.stack((xs,ys)))\n",
    "    classes.append(\"gamma\")\n",
    "for i in range(no_per_class):\n",
    "    ys=[]\n",
    "    #peak=np.random.choice(xs[:-3])\n",
    "    for x in xs:\n",
    "        y=noisy(2)\n",
    "        ys.append(y)\n",
    "    #ys[peak]=noisy(10)\n",
    "    #if peak<xs[-2]:\n",
    "    #ys[peak+2]=ys[peak+3]=noisy(5)\n",
    "    lcs.append(np.stack((xs,ys)))\n",
    "    classes.append(\"delta\")\n",
    "ids=[]\n",
    "for i in range(len(classes)):\n",
    "    ids.append(i)\n",
    "ob_state = {}\n",
    "for i, ob in enumerate(classes):\n",
    "    ob_state[i] = ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test, id_train, id_test = train_test_split(lcs, classes, ids, test_size=0.5, random_state=1, stratify=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ef43db44ace6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mlc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mn_lc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mdistance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msha\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance_calculation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapelet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_abandon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;31m#save the distance value together with the classification and lightcurve id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_lc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbelong_class\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/shapelet_timeseries_classification/shapelets.py\u001b[0m in \u001b[0;36mdistance_calculation\u001b[0;34m(shapelet, lc, time_res, early_abandon)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstart_p\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlc_l\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msha_l\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#length difference+1 will give the number of iterations required to shift the moving windown from start to end of the LC (with a difference of one point, two window positions are required etc.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mend_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_p\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msha_l\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;31m#-1 to give the index of the last included point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_p\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart_p\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msha_l\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtime_res\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m#for \"early abandon\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(sha)\n",
    "best_shapelets=[]\n",
    "time_res=1\n",
    "x=[]\n",
    "for n_donor, lc_donor in enumerate(x_train):\n",
    "    #Create lists with classifications of all time-series relative to the donor time series; one that the pool of shapelets is generated from\n",
    "    state_donor = y_train[n_donor]#y_train[n],x_train[n] and id_train[n] all refer to the attributes of the same time series\n",
    "    belong_class=[]\n",
    "    other_class=[]\n",
    "    for n, i in enumerate(id_train):\n",
    "        if y_train[n] == state_donor:\n",
    "            belong_class.append(i)\n",
    "        else:\n",
    "            other_class.append(i)\n",
    "    #calculate the entropy of the entire set, so it can be compared to the split set later\n",
    "    prop_belong = len(belong_class)/(len(belong_class)+len(other_class))\n",
    "    set_entropy = -(prop_belong)*math.log2(prop_belong)-(1-prop_belong)*math.log2(1-prop_belong)\n",
    "    pool=sha.generate_shapelets(lc_donor, 1, len(lc_donor[0]))#generate shapelets from the donor time-series, \n",
    "    #set the initial best value of information gain to 0 (improved by any split) and start testing the shapelets\n",
    "    best_gain=0\n",
    "    for shapelet in pool:\n",
    "        skip_shapelet=False#for entropy pruning\n",
    "        #set the order of distance calculations\n",
    "        #pick an other_class object first and then alternate between belong and other, when one group runs out, append the rest of the other group to the end\n",
    "        order=[]\n",
    "        if len(belong_class)<len(other_class):alternations=len(belong_class);larger_group=other_class\n",
    "        else: alternations=len(other_class); larger_group=belong_class\n",
    "        for i in range(alternations):\n",
    "            order.append(other_class[i])\n",
    "            order.append(belong_class[i])\n",
    "        for i in range(len(larger_group)-alternations):\n",
    "            order.append(larger_group[-(i+1)])\n",
    "        #start distance calculations\n",
    "        distances=[]\n",
    "        for n_lc in order:\n",
    "            if id_train[n_donor] == n_lc:\n",
    "                distance = 0\n",
    "            else:\n",
    "                lc=x_train[np.where(np.array(id_train)==n_lc)[0][0]]\n",
    "                distance=sha.distance_calculation(shapelet, lc, early_abandon=True)\n",
    "            #save the distance value together with the classification and lightcurve id\n",
    "            if n_lc in belong_class:\n",
    "                class_assign=1\n",
    "            else:\n",
    "                class_assign=0\n",
    "            distances.append((n_lc ,distance, class_assign))\n",
    "            #find the optimal split point if there are at least two distances calculated, then use entropy pruning to find if the shapelet still has a change to beat the best one found so far\n",
    "            if len(distances)>1:\n",
    "                best_split=sha.best_split_point(distances, set_entropy)\n",
    "                #x.append((best_split, distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3 0.6666666666666666 0.6\n",
      "0.9182958340544896 0.9709505944546686\n",
      "-0.13992693484546864\n"
     ]
    }
   ],
   "source": [
    "distances=[[-1,3,0],[-1,10,1],[-1,3,1],[-1,7,1],\n",
    "          [-1,3,0],[-1,10,0],[-1,3,1],[-1,8,1]]\n",
    "prop_belong=(above_belong+below_belong)/len(distances)\n",
    "split_poin=8\n",
    "set_entropy = -(prop_belong)*math.log2(prop_belong)-(1-prop_belong)*math.log2(1-prop_belong)\n",
    "#split_point=sha.best_split_point(distances, set_entropy)\n",
    "above=[lc for lc in distances if lc[1]>=split_point]\n",
    "above_belong=sum([lc[2] for lc in above])\n",
    "below=[lc for lc in distances if lc[1]<split_point]\n",
    "below_belong=sum([lc[2] for lc in below])\n",
    "prop_above_belong=above_belong/len(above)\n",
    "prop_below_belong=below_belong/len(below)\n",
    "if prop_above_belong==1. or prop_above_belong==0.:\n",
    "    above_entropy=0\n",
    "else:\n",
    "    above_entropy = -(prop_above_belong)*math.log2(prop_above_belong)-(1-prop_above_belong)*math.log2(1-prop_above_belong)\n",
    "if prop_below_belong==1. or prop_below_belong==0.:\n",
    "    below_entropy =0\n",
    "else:\n",
    "    below_entropy = -(prop_below_belong)*math.log2(prop_below_belong)-(1-prop_below_belong)*math.log2(1-prop_below_belong)\n",
    "info_gain=set_entropy-(len(above)/(len(above)+len(below)))*(above_entropy)-(len(below)/(len(above)+len(below)))*(below_entropy)\n",
    "print(above_belong, below_belong, prop_above_belong, prop_below_belong)\n",
    "print(above_entropy,below_entropy)\n",
    "print(info_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4 0.6666666666666666 0.8\n",
      "0.9182958340544896 0.7219280948873623\n",
      "0.15886800584993\n"
     ]
    }
   ],
   "source": [
    "print(above_belong, below_belong, prop_above_belong, prop_below_belong)\n",
    "print(above_entropy,below_entropy)\n",
    "print(info_gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8112781244591328"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.9387218755408672"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(len(above)/(len(distances)))*(above_entropy)-(len(below)/(len(distances)))*(below_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(above)+len(below))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(distances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
