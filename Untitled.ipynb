{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:An unexpected error occurred while tokenizing input\n",
      "The following traceback may be corrupted or invalid\n",
      "The error message is: ('EOF in multi-line string', (1, 2))\n",
      "\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named _check_build\n___________________________________________________________________________\nContents of /home/jkok1g14/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/__check_build:\n__init__.pyc              __init__.py               _check_build.cpython-36m-x86_64-linux-gnu.so\nsetup.py                  __pycache__\n___________________________________________________________________________\nIt seems that scikit-learn has not been built correctly.\n\nIf you have installed scikit-learn from source, please do not forget\nto build the package before using it: run `python setup.py install` or\n`make` in the source directory.\n\nIf you have used an installer, please check that it is suited for your\nPython version, your operating system and your platform.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7b825837b782>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfnmatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m  \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jkok1g14/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# process, as it may not be compiled yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[0;31m# avoid flakes unused variable error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jkok1g14/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/__check_build/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_check_build\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_build\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mraise_build_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jkok1g14/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/__check_build/__init__.py\u001b[0m in \u001b[0;36mraise_build_error\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbuild\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpackage\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0musing\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mpython\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m \u001b[0minstall\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m`\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msource\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m %s\"\"\" % (e, local_dir, ''.join(dir_content).strip(), msg))\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named _check_build\n___________________________________________________________________________\nContents of /home/jkok1g14/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/__check_build:\n__init__.pyc              __init__.py               _check_build.cpython-36m-x86_64-linux-gnu.so\nsetup.py                  __pycache__\n___________________________________________________________________________\nIt seems that scikit-learn has not been built correctly.\n\nIf you have installed scikit-learn from source, please do not forget\nto build the package before using it: run `python setup.py install` or\n`make` in the source directory.\n\nIf you have used an installer, please check that it is suited for your\nPython version, your operating system and your platform."
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import shapelets as sha\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "from  sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import csv\n",
    "from sklearn import tree\n",
    "import sys\n",
    "sys.stdout.flush()\n",
    "import math\n",
    "\n",
    "clean_belloni = open('1915Belloniclass_updated.dat')\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "ob_state = {}\n",
    "for state, obs in belloni_clean.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob] = state\n",
    "\n",
    "available = []\n",
    "pool=[]\n",
    "for root, dirnames, filenames in os.walk(\"/home/jkok1g14/Documents/GRS1915+105/data/Std1_PCU2\"):\n",
    "    for filename in fnmatch.filter(filenames, \"*_std1_lc.txt\"):\n",
    "        available.append(filename)\n",
    "for ob, state in ob_state.items():\n",
    "    if ob+\"_std1_lc.txt\" in available:\n",
    "        pool.append(ob)  \n",
    "\n",
    "#create a list of arrays with time and counts for the set of Belloni classified observations\n",
    "lc_dirs=[]\n",
    "lcs=[]\n",
    "ids=[]\n",
    "for root, dirnames, filenames in os.walk(\"/home/jkok1g14/Documents/GRS1915+105/data/Std1_PCU2\"):    \n",
    "    for filename in fnmatch.filter(filenames, \"*_std1_lc.txt\"):\n",
    "        if filename.split(\"_\")[0] in pool:\n",
    "            lc_dirs.append(os.path.join(root, filename))\n",
    "\n",
    "            \n",
    "#make 2D arrays for light curves, with columns of counts and time values\n",
    "for lc in lc_dirs:\n",
    "    ids.append(lc.split(\"/\")[-1].split(\"_\")[0])\n",
    "    f=np.loadtxt(lc)\n",
    "    f=np.transpose(f)#,axis=1)\n",
    "    f=f[0:2]\n",
    "    ###1s average and time check to eliminate points outside of GTIs\n",
    "    f8t = np.mean(f[0][:(len(f[0])//8)*8].reshape(-1, 8), axis=1)\n",
    "    f8c = np.mean(f[1][:(len(f[1])//8)*8].reshape(-1, 8), axis=1)\n",
    "    f8c=f8c/np.max(f8c)\n",
    "    rm_points = []\n",
    "    skip=False\n",
    "    for i in range(len(f8t)-1):\n",
    "        if skip==True:\n",
    "            skip=False\n",
    "            continue\n",
    "        delta = f8t[i+1]-f8t[i]\n",
    "        if delta > 1.0:\n",
    "            rm_points.append(i+1)\n",
    "            skip=True\n",
    "            \n",
    "####### normalise the count rates! think about the effect of 0-1 normalisation on the distance calculation\n",
    "            \n",
    "    times=np.delete(f8t,rm_points)\n",
    "    counts=np.delete(f8c,rm_points)\n",
    "    lcs.append(np.stack((times,counts)))\n",
    "print(len(lcs))\n",
    "lc_classes=[]\n",
    "for i in ids:\n",
    "    lc_classes.append(ob_state[i])\n",
    "lc_classes\n",
    "\n",
    "drop_classes=[]\n",
    "for clas, no in Counter(lc_classes).items():\n",
    "    if no<7:\n",
    "        drop_classes.append(clas)\n",
    "\n",
    "lcs_abu = []\n",
    "classes_abu = []\n",
    "ids_abu = []\n",
    "for n, lc in enumerate(lc_classes):\n",
    "    if lc not in drop_classes:\n",
    "        classes_abu.append(lc)\n",
    "        lcs_abu.append(lcs[n])\n",
    "        ids_abu.append(ids[n])\n",
    "print(len(lcs_abu))        \n",
    "x_train, x_test, y_train, y_test, id_train, id_test = train_test_split(lcs_abu, classes_abu, ids_abu, test_size=0.5, random_state=0, stratify=classes_abu)\n",
    "print(len(lcs_abu), len(classes_abu), len(ids_abu)) \n",
    "\n",
    "best_shapelets=[]\n",
    "time_res=1\n",
    "for n_donor, lc_donor in enumerate(x_train):\n",
    "    #Create lists with classifications of all time-series relative to the donor time series; one that the pool of shapelets is generated from\n",
    "    state_donor = y_train[n_donor]\n",
    "    belong_class=[]\n",
    "    other_class=[]\n",
    "    for n, i in enumerate(id_train):\n",
    "        if y_train[n] == state_donor:\n",
    "            belong_class.append(i)\n",
    "        else:\n",
    "            other_class.append(i)\n",
    "    #calculate the entropy of the entire set, so it can be compared to the split set later\n",
    "    prop_belong = len(belong_class)/(len(belong_class)+len(other_class))\n",
    "    set_entropy = -(prop_belong)*math.log(prop_belong, 2)-(1-prop_belong)*math.log(1-prop_belong, 2)\n",
    "    pool=sha.generate_shapelets(lc_donor, 1, len(lc_donor[0]))#generate shapelets from the donor time-series, \n",
    "    #set the initial best value of information gain to 0 (improved by any split) and start testing the shapelets\n",
    "    best_gain=0\n",
    "    print(state_donor)\n",
    "    for shapelet in pool:\n",
    "        skip_shapelet=False#for entropy pruning\n",
    "        #set the order of distance calculations\n",
    "        #pick an other_class object first and then alternate between belong and other, when one group runs out, append the rest of the other group to the end\n",
    "        order=[]\n",
    "        if len(belong_class)<len(other_class):alternations=len(belong_class);larger_group=other_class\n",
    "        else: alternations=len(other_class); larger_group=belong_class\n",
    "        for i in range(alternations):\n",
    "            order.append(other_class[i])\n",
    "            order.append(belong_class[i])\n",
    "        for i in range(len(larger_group)-alternations):\n",
    "            order.append(larger_group[-(i+1)])\n",
    "        #start distance calculations\n",
    "        distances=[]\n",
    "        for n_lc in order:\n",
    "            if id_train[n_donor] == n_lc:\n",
    "                distance = 0\n",
    "            else:\n",
    "                lc=x_train[np.where(np.array(id_train)==n_lc)[0][0]]\n",
    "                distance=sha.distance_calculation(shapelet, lc, early_abandon=True)\n",
    "            #save the distance value together with the classification and lightcurve id\n",
    "            if n_lc in belong_class:\n",
    "                class_assign=1\n",
    "            else:\n",
    "                class_assign=0\n",
    "            distances.append((n_lc ,distance, class_assign))\n",
    "            #find the optimal split point if there are at least two distances calculated, then use entropy pruning to find if the shapelet still has a change to beat the best one found so far\n",
    "            if len(distances)>1:\n",
    "                best_split=sha.best_split_point(distances, set_entropy)\n",
    "                skip_shapelet=sha.entropy_pruning(best_gain, distances, best_split, len(belong_class), len(other_class), set_entropy)\n",
    "                if skip_shapelet==True:\n",
    "                    break\n",
    "        #if shapelet was not rejected at entropy pruning, calculcate the information gain and if the value is larger then the best one so far, save the shapelet\n",
    "        if skip_shapelet==False:\n",
    "            gain=sha.information_gain(distances, set_entropy, best_split)\n",
    "            if gain>best_gain:\n",
    "                best_gain=gain\n",
    "                best_shapelet=shapelet\n",
    "    print((best_shapelet, best_split, best_gain, id_train[n_donor], state_donor))\n",
    "    best_shapelets.append((best_shapelet, best_split, best_gain, id_train[n_donor], state_donor))\n",
    "    break\n",
    "train_dists=np.zeros((len(x_train),len(best_shapelets)))\n",
    "for i_t, t in enumerate(x_train):\n",
    "    for i_s, s in enumerate(best_shapelets):\n",
    "        distance=sha.distance_calculation(s[0], t, early_abandon=False)\n",
    "        train_dists[i_t,i_s]=distance\n",
    "print(\"train_dists\\n\", train_dists)\n",
    "\n",
    "test_dists=np.zeros((len(x_test),len(best_shapelets)))\n",
    "for i_t, t in enumerate(x_test):\n",
    "    for i_s, s in enumerate(best_shapelets):\n",
    "        distance=sha.distance_calculation(s[0], t, early_abandon=False)\n",
    "        test_dists[i_t,i_s]=distance\n",
    "print(\"test_dists\\n\",test_dists)\n",
    "\n",
    "\n",
    "dtc=tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "dtc.fit(train_dists, y_train)\n",
    "train_inference= dtc.score(train_dists)\n",
    "print(\"train_inference_score\\n\",train_inference)\n",
    "\n",
    "dtc.fit(train_dists, y_train)\n",
    "inference= dtc.predict(test_dists)\n",
    "print(\"test_inference\\n\", inference)\n",
    "score=[]\n",
    "for n, i in enumerate(inference):\n",
    "    if i ==y_test[n]:\n",
    "        score.append(1)\n",
    "    else:\n",
    "        score.append(0)\n",
    "score=np.array(score)\n",
    "print(\"test_inference_score\\n\",np.mean(score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "future feature sklearn is not defined (<ipython-input-3-b8679bc5cdd3>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-b8679bc5cdd3>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    from __future__ import sklearn\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m future feature sklearn is not defined\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import sklearn\n",
    "import math\n",
    "belong_class=8\n",
    "other_class=93\n",
    "prop_belong = belong_class/(belong_class+other_class)\n",
    "print(prop_belong, belong_class, other_class)\n",
    "set_entropy = -(prop_belong)*math.log(prop_belong, 2)-(1-prop_belong)*math.log((1-prop_belong), 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
