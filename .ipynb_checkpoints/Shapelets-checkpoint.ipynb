{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "import shapelets as sha\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the list of Belloni classified observations to focus on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_belloni = open('1915Belloniclass_updated.dat')\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "belloni_files=[]\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "for state, obs in belloni_clean.items():\n",
    "    for obID in obs:\n",
    "        belloni_files.append(obID+\"_std1_lc.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all available light curves. Shapelets should not span more than 1 GTI, so it should be checked what the length of the shortest GTI in the data set is and set the max length of the shapelet to this value (or filter out the observations with only short GTI (if there are not too many)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gti_dirs=[]\n",
    "gti=[]\n",
    "inters=[]\n",
    "for root, dirnames, filenames in os.walk(\"/home/jkok1g14/Documents/GRS1915+105/data/Std1_PCU2\"):\n",
    "    for filename in fnmatch.filter(filenames, \"std1.gti\"):\n",
    "        gti_dirs.append(os.path.join(root, filename))\n",
    "for f in gti_dirs:\n",
    "    with fits.open(f) as hdulist:\n",
    "        for inter in hdulist[1].data:\n",
    "            inters.append(inter[1]-inter[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95.84% of GTIs are longer or equal to 500 seconds\n",
    "\n",
    "Arrays of Time and Counts of every light curve appended to lcs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10218\n"
     ]
    }
   ],
   "source": [
    "lc_dirs=[]\n",
    "times=[]\n",
    "counts=[]\n",
    "ids=[]\n",
    "for root, dirnames, filenames in os.walk(\"/home/jkok1g14/Documents/GRS1915+105/data/Std1_PCU2\"):\n",
    "    for filename in fnmatch.filter(filenames, \"*_std1_lc.txt\"):\n",
    "        if filename in belloni_files:\n",
    "            lc_dirs.append(os.path.join(root, filename))\n",
    "for lc in lc_dirs:\n",
    "    ids.append(lc.split(\"/\")[-1].split(\"_\")[0])\n",
    "    f=np.loadtxt(lc)\n",
    "    f=np.transpose(f)#,axis=1)\n",
    "    f=f[0:2]\n",
    "    ###1s average and time check to eliminate points outside of GTIs\n",
    "    f8t = np.mean(f[0][:(len(f[0])//8)*8].reshape(-1, 8), axis=1)\n",
    "    f8c = np.mean(f[1][:(len(f[1])//8)*8].reshape(-1, 8), axis=1)\n",
    "    print(len(f8t))\n",
    "    rm_points = []\n",
    "    skip=False\n",
    "    for i in range(len(f8t)-1):\n",
    "        if skip==True:\n",
    "            skip=False\n",
    "            continue\n",
    "        delta = f8t[i+1]-f8t[i]\n",
    "        if delta > 1.0:\n",
    "            rm_points.append(i+1)\n",
    "            skip=True\n",
    "    times=np.delete(f8t,rm_points)\n",
    "    counts=np.delete(f8c,rm_points)\n",
    "with open('light_curves.txt', 'w') as f:\n",
    "    for i, lc in enumerate(ids)\n",
    "        f.write(lc)\n",
    "        f.write(times[i])\n",
    "        f.write(counts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-214-45a0e5b961e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlcs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0.125\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;31m#convert from seconds to data points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0msh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0minter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pool=[]\n",
    "maxlen=500\n",
    "minlen=20\n",
    "for l in range(minlen,maxlen+1):\n",
    "    for lc in lcs:\n",
    "        end=int(l/0.125+1); start=0 #convert from seconds to data points\n",
    "        while end<=len(lc[0]):\n",
    "            sh=lc[1,start:end]\n",
    "            inter=(lc[0,end-1]-lc[0,start])\n",
    "            end+=1; start+=1\n",
    "            if inter==l:\n",
    "                pool.append(sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9fb15b994e53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlc_dirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#,axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding)\u001b[0m\n\u001b[1;32m   1090\u001b[0m         \u001b[0;31m# converting the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1092\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_loadtxt_chunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1093\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1094\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(chunk_size)\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_line\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m             \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msplit_line\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregex_comments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.stdout.flush()\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "#create a belloni_files list with names of files holding Belloni classified lightr curves\n",
    "clean_belloni = open('1915Belloniclass_updated.dat')\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "belloni_files=[]\n",
    "belloni_files_dict = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "for state, obs in belloni_clean.items():\n",
    "    file_list=[]\n",
    "    for obID in obs:\n",
    "        file_list.append(obID)\n",
    "        belloni_files.append(obID+\"_std1_lc.txt\")\n",
    "    belloni_files_dict[state] = file_list\n",
    "    #state: file1, file2...\n",
    "ob_state = {}\n",
    "for state, obs in belloni_files_dict.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob] = state\n",
    "#xp10408013100_lc.txt classified as chi1 and chi4, xp20402011900_lc.txt as chi2 and chi2\n",
    "#del ob_state[\"10408-01-31-00{}\".format(extension)] as long as training and test sets are checked for duplicates when appending, it should be ok to keep\n",
    "available = []\n",
    "pool=[]\n",
    "for root, dirnames, filenames in os.walk(\"/home/jkok1g14/Documents/GRS1915+105/data/Std1_PCU2\"):\n",
    "    for filename in fnmatch.filter(filenames, \"*_std1_lc.txt\"):\n",
    "        available.append(filename)\n",
    "for ob, state in ob_state.items():\n",
    "    if ob+\"_std1_lc.txt\" in available:\n",
    "        pool.append(ob)        \n",
    "\n",
    "training_obs = []\n",
    "training_states = []\n",
    "test_obs = []\n",
    "test_states = []\n",
    "randomize = np.random.choice(list(ob_state.keys()), len(ob_state.keys()), replace=False)\n",
    "for ob in randomize:\n",
    "    state = ob_state[\"{}\".format(ob)]\n",
    "    if state not in training_states:\n",
    "        if ob in pool:\n",
    "            training_obs.append(ob)\n",
    "            training_states.append(state)\n",
    "no_train = math.ceil(len(pool)*0.50)\n",
    "for ob in training_obs:\n",
    "    pool.remove(\"{}\".format(ob))        \n",
    "for ob in randomize:\n",
    "    state = ob_state[\"{}\".format(ob)]\n",
    "    if state not in test_states:\n",
    "        if ob in pool:\n",
    "            test_obs.append(ob)\n",
    "            test_states.append(state)\n",
    "for ob in test_obs:\n",
    "    pool.remove(\"{}\".format(ob))\n",
    "remaining = int(no_train-len(training_obs))\n",
    "train_remain = np.random.choice(pool, size = remaining, replace=False)\n",
    "for ob in train_remain:\n",
    "    training_obs.append(ob)\n",
    "# test_obs = []\n",
    "for ob in pool:\n",
    "    if ob not in training_obs:\n",
    "        test_obs.append(ob)\n",
    "        \n",
    "        \n",
    "#create a list of arrays with time and counts for the set of Belloni classified observations\n",
    "lc_dirs=[]\n",
    "lcs=[]\n",
    "ids=[]\n",
    "#for root, dirnames, filenames in os.walk(\"/export/data/jakubok/GRS1915+105/Std1_PCU2\"):\n",
    "for root, dirnames, filenames in os.walk(\"/home/jkok1g14/Documents/GRS1915+105/data/Std1_PCU2\"):    \n",
    "    for filename in fnmatch.filter(filenames, \"*_std1_lc.txt\"):\n",
    "        if filename in belloni_files:\n",
    "            lc_dirs.append(os.path.join(root, filename))\n",
    "# extract shapelets from one LC, test the class, save the best shapelet and distance, replace shapelets with the next LC\n",
    "for lc in lc_dirs:\n",
    "    ids.append(lc.split(\"/\")[-1].split(\"_\")[0])\n",
    "    f=np.loadtxt(lc)\n",
    "    f=np.transpose(f)#,axis=1)\n",
    "    f=f[0:2]\n",
    "    ###1s average and time check to eliminate points outside of GTIs\n",
    "    f8t = np.mean(f[0][:(len(f[0])//8)*8].reshape(-1, 8), axis=1)\n",
    "    f8c = np.mean(f[1][:(len(f[1])//8)*8].reshape(-1, 8), axis=1)\n",
    "    rm_points = []\n",
    "    skip=False\n",
    "    for i in range(len(f8t)-1):\n",
    "        if skip==True:\n",
    "            skip=False\n",
    "            continue\n",
    "        delta = f8t[i+1]-f8t[i]\n",
    "        if delta > 1.0:\n",
    "            rm_points.append(i+1)\n",
    "            skip=True\n",
    "    times=np.delete(f8t,rm_points)\n",
    "    counts=np.delete(f8c,rm_points)\n",
    "    lcs.append(np.stack((times,counts)))\n",
    "print(\"No. of light curves prepared: {}\".format(len(lcs)))\n",
    "#create a pool of shapelets for the set of light curves\n",
    "pool=[]\n",
    "maxlen=500\n",
    "minlen=20\n",
    "pool=sha.generate_shapelets(lc, 20, 500)\n",
    "for n, lc in enumerate(lcs):\n",
    "    for l in range(minlen,maxlen+1):\n",
    "        end=int(l+1); start=0\n",
    "        while end<=len(lc[0]):\n",
    "            sh=lc[1,start:end]\n",
    "            inter=(lc[0,end-1]-lc[0,start])\n",
    "            end+=1; start+=1\n",
    "            if inter==l:\n",
    "                pool.append(sh)\n",
    "#print(\"No. of shapelets prepared: {}\".format(len(pool)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[7.61596198e+07, 7.61596208e+07, 7.61596218e+07, ...,\n",
       "         7.61791877e+07, 7.61791887e+07, 7.61791897e+07],\n",
       "        [1.67000000e+03, 1.66700000e+03, 1.82100000e+03, ...,\n",
       "         2.11900000e+03, 1.89700000e+03, 2.03200000e+03]])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4413175"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.91795164"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total=0\n",
    "for l in pool:\n",
    "    total+=l.nbytes\n",
    "total*(1.e-9)\n",
    "#print(sys.getsizeof(pool[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of light curves prepared: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clean_belloni = open('1915Belloniclass_updated.dat')\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "belloni_files=[]\n",
    "belloni_files_dict = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "for state, obs in belloni_clean.items():\n",
    "    file_list=[]\n",
    "    for obID in obs:\n",
    "        file_list.append(obID)\n",
    "        belloni_files.append(obID+\"_std1_lc.txt\")\n",
    "    belloni_files_dict[state] = file_list\n",
    "ob_state = {}\n",
    "for state, obs in belloni_files_dict.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob = ob.split(\"_\")[0]\n",
    "        ob_state[ob] = state\n",
    "#create a list of arrays with time and counts for the set of Belloni classified observations\n",
    "lc_dirs=[]\n",
    "lcs=[]\n",
    "ids=[]\n",
    "#for root, dirnames, filenames in os.walk(\"/export/data/jakubok/GRS1915+105/Std1_PCU2\"):\n",
    "for root, dirnames, filenames in os.walk(\"/home/jkok1g14/Documents/GRS1915+105/data/Std1_PCU2\"):    \n",
    "    for filename in fnmatch.filter(filenames, \"*_std1_lc.txt\"):\n",
    "        if filename in belloni_files:\n",
    "            lc_dirs.append(os.path.join(root, filename))\n",
    "# extract shapelets from one LC, test the class, save the best shapelet and distance, replace shapelets with the next LC\n",
    "for lc in lc_dirs:\n",
    "    ids.append(lc.split(\"/\")[-1].split(\"_\")[0])\n",
    "    f=np.loadtxt(lc)\n",
    "    f=np.transpose(f)#,axis=1)\n",
    "    f=f[0:2]\n",
    "    ###1s average and time check to eliminate points outside of GTIs\n",
    "    f8t = np.mean(f[0][:(len(f[0])//8)*8].reshape(-1, 8), axis=1)\n",
    "    f8c = np.mean(f[1][:(len(f[1])//8)*8].reshape(-1, 8), axis=1)\n",
    "    rm_points = []\n",
    "    skip=False\n",
    "    for i in range(len(f8t)-1):\n",
    "        if skip==True:\n",
    "            skip=False\n",
    "            continue\n",
    "        delta = f8t[i+1]-f8t[i]\n",
    "        if delta > 1.0:\n",
    "            rm_points.append(i+1)\n",
    "            skip=True\n",
    "            \n",
    "####### normalise the count rates! think about the effect of 0-1 normalisation on the distance calculation\n",
    "            \n",
    "    times=np.delete(f8t,rm_points)\n",
    "    counts=np.delete(f8c,rm_points)\n",
    "    lcs.append(np.stack((times,counts)))\n",
    "    break\n",
    "print(\"No. of light curves prepared: {}\".format(len(lcs)))\n",
    "#create a pool of shapelets for the set of light curves\n",
    "\n",
    "#it would likely be necessary to test the pool of shapelets once per light curve so that the pool would not take up too much memory \n",
    "time_res=1\n",
    "for n_donor, lc_donor in enumerate(lcs):\n",
    "    state_donor = ob_state[ids[n_donor]]\n",
    "    for n_lc, lc in enumerate(lcs):\n",
    "    if ob_state[ids[n_lc]] == state_donor:\n",
    "        belong_class.append(n_lc)\n",
    "    else:\n",
    "        other_class.append(n_lc)\n",
    "    pool=sha.generate_shapelets(lc_donor, 20, 500)\n",
    "    for shapelet in pool:\n",
    "        #gain=sha.information_gain(shapelet, set of time-series)\n",
    "        belong_class = []\n",
    "        other_class = []\n",
    "        sha_l = len(shapelet)\n",
    "        distances=[]\n",
    "\n",
    "        prop_belong = len(belong_class)/(len(belong_class)+len(other_class))\n",
    "        set_entropy = -(prop_belong)*math.log(prop_belong)-(1-prop_belong)*math.log(1-prop_belong)\n",
    "        \n",
    "        alternator=1\n",
    "        for i in range(len(lcs)):\n",
    "            if alternator ==1:\n",
    "                alternator*=-1\n",
    "                #use an other_class sample\n",
    "            else:\n",
    "                alternator*=-1\n",
    "                #use a belong_class sample\n",
    "        \n",
    "        \n",
    "        for n_lc, lc in enumerate(lcs):\n",
    "            best_dist=np.inf #for \"early abandon\"\n",
    "            lc_l = len(lc)\n",
    "            skip=False\n",
    "            for start_p in range(lc_l-sha_l+1)#length difference+1 will give the number of iterations required to shift the moving windown from start to end of the LC (with a difference of one point, two window positions are required etc.)\n",
    "                if skip == True: skip=False\n",
    "                    continue  \n",
    "                end_p=start_p+sha_l-1 #-1 to give the index of the last included point\n",
    "                if lc[0,end_p]-lc[0,start_p] != (sha_l-1)*time_res:\n",
    "                    continue\n",
    "                sha_dist=0 #distance between shapelet and LC subsegment\n",
    "                for i in range(sha_l):\n",
    "                    sha_dist += (lc[1,i+]-shapelet[i])**2\n",
    "                    if sha_dist>=best_dist: skip=True#\"early abandon\"\n",
    "                        break#break out of the distance calculation and skip the position of the moving window\n",
    "                if skip ==False:\n",
    "                    best_dist=sha_dist\n",
    "            \n",
    "            distances.append((n_lc, best_dist))\n",
    "            distances.sort(key=itemgetter(1))\n",
    "            \n",
    "        best_gain_splits=0\n",
    "        for split_point in range(len(distances)-1):\n",
    "            point=(distances[split_point][1] + distances[split_point+1][1])/2\n",
    "            above_belong=sum([1 for lc in distances if lc[1]>point and n_lc in belong_class])\n",
    "            below_belong=sum([1 for lc in distances if lc[1]<point and n_lc in belong_class])\n",
    "            above_other=sum([1 for lc in distances if lc[1]>point and n_lc in other_class])\n",
    "            below_other=sum([1 for lc in distances if lc[1]<point and n_lc in other_class])\n",
    "            prop_above_belong=above_belong/(above_belong+above_other)\n",
    "            prop_below_belong=below_belong/(below_belong+below_other)\n",
    "            above_entropy = -(prop_above_belong)*math.log(prop_above_belong)-(1-prop_above_belong)*math.log(1-prop_above_belong)\n",
    "            below_entropy = -(prop_below_belong)*math.log(prop_below_belong)-(1-prop_below_belong)*math.log(1-prop_below_belong)\n",
    "            gain=set_entropy-(above_belong+above_other)*(above_entropy)+(below_belong+below_other)*(below_entropy)\n",
    "            if gain>best_gain_splits:\n",
    "                best_gain=gain\n",
    "                best_split=split_point\n",
    "            \n",
    "                    \n",
    "    break\n",
    "#print(\"No. of shapelets prepared: {}\".format(len(pool)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_shapelets=[]\n",
    "time_res=1\n",
    "for n_donor, lc_donor in enumerate(lcs):\n",
    "    state_donor = ob_state[ids[n_donor]]\n",
    "    for n_lc in range(len(lcs)):\n",
    "    if ob_state[ids[n_lc]] == state_donor:\n",
    "        belong_class.append(n_lc)\n",
    "    else:\n",
    "        other_class.append(n_lc)\n",
    "    prop_belong = len(belong_class)/(len(belong_class)+len(other_class))\n",
    "    set_entropy = -(prop_belong)*math.log(prop_belong)-(1-prop_belong)*math.log(1-prop_belong)\n",
    "    pool=sha.generate_shapelets(lc_donor, 20, 500)\n",
    "    best_gain=0\n",
    "    skip_shapelet=False #for entropy pruning\n",
    "    for shapelet in pool:\n",
    "        if skip_shapelet==True: skip_shapelet=False\n",
    "            continue\n",
    "        distances=[]\n",
    "        for n_lc, lc in enumerate(lcs):\n",
    "            distance=sha.distance_calculation(n_lc, lc, shapelet, time_res, belong_class)\n",
    "            distances.append(distance)\n",
    "            if len(distances)>1:\n",
    "                best_split=sha.best_split_point(distances, set_entropy)\n",
    "                \n",
    "                skip_shapelet=sha.entropy_pruning(best_gain, distances, best_split, len(belong_class), len(other_class))\n",
    "                if skip_shapelet==True:\n",
    "                    break\n",
    "        gain=sha.information_gain(distances, set_entropy, split_point):\n",
    "        if gain>best_gain:\n",
    "            best_gain=gain\n",
    "            best_shapelet=shapelet\n",
    "    best_shapelets.append((best_shapelet, best_gain, state_donor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_pruning(best_gain, distances, best_split, belong_class_count, other_class_count, set_entropy):\n",
    "    calc_belong=sum([lc[2] for lc in distances])\n",
    "    calc_other=len(distances)-calc_belong\n",
    "    distances_bcs=distances #best case scenario when all the distances are included\n",
    "    distances_bcs.sort(key=itemgetter(1))\n",
    "    maxdist=distances1[-1][1]\n",
    "    for add_belong in range(belong_class_count-calc_belong):\n",
    "        distances_bcs.append((-1,0,1))\n",
    "    for add_other in range(other_class_count-calc_other):\n",
    "        distances_bcs.append((-1,maxdist,0))\n",
    "    gain_bcs=sha.information_gain(distances_bcs, set_entropy, best_split)\n",
    "    if gain_bcs>best_gain:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_calculation(n_lc, lc, shapelet, time_res, belong_class, early_abandon=True):\n",
    "    if early_abandon ==True:\n",
    "        best_dist=np.inf #for \"early abandon\"\n",
    "        lc_l = len(lc)\n",
    "        skip=False\n",
    "        for start_p in range(lc_l-sha_l+1)#length difference+1 will give the number of iterations required to shift the moving windown from start to end of the LC (with a difference of one point, two window positions are required etc.)\n",
    "            if skip == True: skip=False\n",
    "                continue  \n",
    "            end_p=start_p+sha_l-1 #-1 to give the index of the last included point\n",
    "            if lc[0,end_p]-lc[0,start_p] != (sha_l-1)*time_res:\n",
    "                continue\n",
    "            sha_dist=0 #distance between shapelet and LC subsegment\n",
    "            for i in range(sha_l):\n",
    "                sha_dist += (lc[1,i+start_p]-shapelet[i])**2\n",
    "                if sha_dist>=best_dist: skip=True#\"early abandon\"\n",
    "                    break#break out of the distance calculation and skip the position of the moving window\n",
    "            if skip ==False:\n",
    "                best_dist=sha_dist\n",
    "        if n_lc in belong_class:\n",
    "            class_assign=1\n",
    "        else:\n",
    "            class_assign=0\n",
    "return (n_lc, best_dist, class_assign)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(distances, set_entropy, split_point):\n",
    "    above=[lc for lc in distances if lc[1]>split_point]\n",
    "    above_belong=sum([lc[2] for lc in above])\n",
    "    below=[lc for lc in distances if lc[1]<split_point]\n",
    "    below_belong=sum([lc[2] for lc in below])\n",
    "    prop_above_belong=above_belong/len(above)\n",
    "    prop_below_belong=below_belong/len(below)\n",
    "    above_entropy = -(prop_above_belong)*math.log(prop_above_belong)-(1-prop_above_belong)*math.log(1-prop_above_belong)\n",
    "    below_entropy = -(prop_below_belong)*math.log(prop_below_belong)-(1-prop_below_belong)*math.log(1-prop_below_belong)\n",
    "return gain=set_entropy-(len(above)/(len(above)+len(below)))*(above_entropy)+(len(below)/(len(above)+len(below)))*(below_entropy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_split_point(distances, set_entropy):\n",
    "    distances.sort(key=itemgetter(1))\n",
    "    best_gain_splits=0\n",
    "    for distance in range(len(distances)-1):\n",
    "        split_point=(distances[distance][1] + distances[distance+1][1])/2\n",
    "        gain=sha.information_gain(distances, set_entropy, split_point)\n",
    "        if gain>best_gain_splits:\n",
    "            best_gain=gain\n",
    "            best_split=split_point\n",
    "return best_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-50b0f7553b27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprop_belong\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbelong_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbelong_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprop_other\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mprop_belong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mset_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop_belong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop_belong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop_other\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop_other\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mset_entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "import math\n",
    "belong_class=[0,6,7,8,9,0,6,7,8,9,0,6,7,8,9,0,6,7,8,9,0,6,7,8,9,0]\n",
    "other_class=[]\n",
    "prop_belong = len(belong_class)/(len(belong_class)+len(other_class))\n",
    "prop_other = 1-prop_belong\n",
    "set_entropy = -(prop_belong)*math.log(prop_belong)-(prop_other)*math.log(prop_other)\n",
    "set_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "import shapelets as sha\n",
    "import importlib\n",
    "importlib.reload(sha)\n",
    "from operator import itemgetter\n",
    "lcs=[]\n",
    "classes=[]\n",
    "xs=np.arange(0,30)\n",
    "def noisy(average_value):\n",
    "    return average_value+(math.cos(random.randint(0,360)*(math.pi/180))*0.01*average_value)\n",
    "no_per_class=3\n",
    "for i in range(no_per_class):\n",
    "    ys=[]\n",
    "    peak=np.random.choice(xs[1:-1])\n",
    "    for x in xs:\n",
    "        y=noisy(2)\n",
    "        ys.append(y)\n",
    "    ys[peak]=ys[peak-1]=ys[peak+1]=noisy(5)\n",
    "    lcs.append(np.stack((xs,ys)))\n",
    "    classes.append(\"alpha\")\n",
    "for i in range(no_per_class):\n",
    "    ys=[]\n",
    "    peak=np.random.choice(xs[:-1])\n",
    "    for x in xs:\n",
    "        y=noisy(2)\n",
    "        ys.append(y)\n",
    "    ys[peak]=noisy(10)\n",
    "    lcs.append(np.stack((xs,ys)))\n",
    "    classes.append(\"beta\")\n",
    "for i in range(no_per_class):\n",
    "    ys=[]\n",
    "    peak=np.random.choice(xs[:-3])\n",
    "    for x in xs:\n",
    "        y=noisy(2)\n",
    "        ys.append(y)\n",
    "    ys[peak]=noisy(10)\n",
    "    #if peak<xs[-2]:\n",
    "    ys[peak+2]=ys[peak+3]=noisy(5)\n",
    "    lcs.append(np.stack((xs,ys)))\n",
    "    classes.append(\"gamma\")\n",
    "for i in range(20):\n",
    "    ys=[]\n",
    "    #peak=np.random.choice(xs[:-3])\n",
    "    for x in xs:\n",
    "        y=noisy(2)\n",
    "        ys.append(y)\n",
    "    #ys[peak]=noisy(10)\n",
    "    #if peak<xs[-2]:\n",
    "    #ys[peak+2]=ys[peak+3]=noisy(5)\n",
    "    lcs.append(np.stack((xs,ys)))\n",
    "    classes.append(\"delta\")\n",
    "ids=[]\n",
    "for i in range(len(classes)):\n",
    "    ids.append(i)\n",
    "ob_state = {}\n",
    "for i, ob in enumerate(classes):\n",
    "    ob_state[i] = ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([1.98043705, 2.01285575, 5.025     ]), 0.4798320236161285, 'alpha'),\n",
       " (array([10.03746066,  2.01231323,  2.01312118]), 0.4798320236161285, 'beta'),\n",
       " (array([1.98217987, 4.98454915]), 0.4798320236161285, 'gamma'),\n",
       " (array([2.01975377, 1.99      , 1.99618382, 2.01854368, 1.98024623,\n",
       "         1.98768677, 2.01312118, 2.01363997, 2.00415823, 1.99448725,\n",
       "         1.99415257, 2.01997259, 1.99283264, 2.01677341, 2.01509419,\n",
       "         1.98768677, 2.        , 2.01175571, 2.01338261, 1.99930201,\n",
       "         2.00209057, 1.98741359, 2.00243739, 2.01969616, 2.00969619]),\n",
       "  0.8935711016541907,\n",
       "  'delta')]"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tested_classes=[]\n",
    "best_shapelets=[]\n",
    "time_res=1\n",
    "for n_donor, lc_donor in enumerate(lcs):\n",
    "    #Create lists with classifications of all time-series relative to the donor time series; one that the pool of shapelets is generated from\n",
    "    state_donor = ob_state[ids[n_donor]]\n",
    "    if state_donor not in tested_classes:\n",
    "        tested_classes.append(state_donor)\n",
    "    else:\n",
    "        continue\n",
    "    belong_class=[]\n",
    "    other_class=[]\n",
    "    for n_lc in range(len(lcs)):\n",
    "        if ob_state[ids[n_lc]] == state_donor:\n",
    "            belong_class.append(n_lc)\n",
    "        else:\n",
    "            other_class.append(n_lc)\n",
    "    #calculate the entropy of the entire set, so it can be compared to the split set later\n",
    "    prop_belong = len(belong_class)/(len(belong_class)+len(other_class))\n",
    "    set_entropy = -(prop_belong)*math.log2(prop_belong)-(1-prop_belong)*math.log2(1-prop_belong)\n",
    "    pool=sha.generate_shapelets(lc_donor, 1, len(lc_donor[0]))#generate shapelets from the donor time-series, \n",
    "    best_gain=0#set the initial best value of information gain to 0 (improved by any split) \n",
    "    for shapelet in pool:\n",
    "        skip_shapelet=False#for entropy pruning\n",
    "        #set the order of distance calculations\n",
    "        #pick an other_class object first and then alternate between belong and other, when one group runs out, append the rest of the other group to the end\n",
    "        order=[]\n",
    "        if len(belong_class)<len(other_class):alternations=len(belong_class);larger_group=other_class\n",
    "        else: alternations=len(other_class); larger_group=belong_class\n",
    "        for i in range(alternations):\n",
    "            order.append(other_class[i])\n",
    "            order.append(belong_class[i])\n",
    "        for i in range(len(larger_group)-alternations):\n",
    "            order.append(larger_group[-(i+1)])\n",
    "        #start distance calculations\n",
    "        distances=[]\n",
    "        for n_lc in order:\n",
    "            lc=lcs[n_lc]\n",
    "            distance=sha.distance_calculation(n_lc, lc, shapelet, time_res, belong_class)\n",
    "           # print(n_donor, distance, shapelet)\n",
    "            distances.append(distance)\n",
    "            if len(distances)>1:\n",
    "                best_split=sha.best_split_point(distances, set_entropy)\n",
    "                skip_shapelet=sha.entropy_pruning(best_gain, distances, best_split, len(belong_class), len(other_class), set_entropy)\n",
    "                if skip_shapelet==True:\n",
    "                    break\n",
    "        if skip_shapelet==False:\n",
    "            gain=sha.information_gain(distances, set_entropy, best_split)\n",
    "            #print(shapelet)\n",
    "            #print(distances)\n",
    "            if gain>best_gain:\n",
    "                best_gain=gain\n",
    "                best_shapelet=shapelet\n",
    "    best_shapelets.append((best_shapelet, best_gain, state_donor))\n",
    "best_shapelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-282-95e974693151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlc_dirs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#,axis=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_loadtxt_chunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 \u001b[0mnshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.stdout.flush()\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import shapelets as sha\n",
    "from operator import itemgetter\n",
    "\n",
    "#create a belloni_files list with names of files holding Belloni classified light curves\n",
    "clean_belloni = open('1915Belloniclass_updated.dat')\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "belloni_files=[]\n",
    "belloni_files_dict = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "for state, obs in belloni_clean.items():\n",
    "    file_list=[]\n",
    "    for obID in obs:\n",
    "        file_list.append(obID)\n",
    "        belloni_files.append(obID+\"_std1_lc.txt\")\n",
    "    belloni_files_dict[state] = file_list\n",
    "    #state: file1, file2...\n",
    "ob_state = {}\n",
    "for state, obs in belloni_clean.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob] = state\n",
    "    #file1: state\n",
    "#xp10408013100_lc.txt classified as chi1 and chi4, xp20402011900_lc.txt as chi2 and chi2\n",
    "#del ob_state[\"10408-01-31-00{}\".format(extension)] as long as training and test sets are checked for duplicates when appending, it should be ok to keep\n",
    "\n",
    "\n",
    "available = []\n",
    "pool=[]\n",
    "for root, dirnames, filenames in os.walk(\"/home/jkok1g14/Documents/GRS1915+105/data/Std1_PCU2\"):\n",
    "    for filename in fnmatch.filter(filenames, \"*_std1_lc.txt\"):\n",
    "        available.append(filename)\n",
    "for ob, state in ob_state.items():\n",
    "    if ob+\"_std1_lc.txt\" in available:\n",
    "        pool.append(ob)        \n",
    "training_obs = []\n",
    "training_states = []\n",
    "test_obs = []\n",
    "test_states = []\n",
    "randomize = np.random.choice(list(ob_state.keys()), len(ob_state.keys()), replace=False)\n",
    "for ob in randomize:\n",
    "    state = ob_state[\"{}\".format(ob)]\n",
    "    if state not in training_states:\n",
    "        if ob in pool:\n",
    "            training_obs.append(ob)\n",
    "            training_states.append(state)\n",
    "no_train = math.ceil(len(pool)*0.50)\n",
    "for ob in training_obs:\n",
    "    pool.remove(\"{}\".format(ob))        \n",
    "for ob in randomize:\n",
    "    state = ob_state[\"{}\".format(ob)]\n",
    "    if state not in test_states:\n",
    "        if ob in pool:\n",
    "            test_obs.append(ob)\n",
    "            test_states.append(state)\n",
    "for ob in test_obs:\n",
    "    pool.remove(\"{}\".format(ob))\n",
    "remaining = int(no_train-len(training_obs))\n",
    "train_remain = np.random.choice(pool, size = remaining, replace=False)\n",
    "for ob in train_remain:\n",
    "    training_obs.append(ob)\n",
    "# test_obs = []\n",
    "for ob in pool:\n",
    "    if ob not in training_obs:\n",
    "        test_obs.append(ob)\n",
    "        \n",
    "        \n",
    "#create a list of arrays with time and counts for the set of Belloni classified observations\n",
    "lc_dirs=[]\n",
    "lcs=[]\n",
    "ids=[]\n",
    "#for root, dirnames, filenames in os.walk(\"/export/data/jakubok/GRS1915+105/Std1_PCU2\"):\n",
    "for root, dirnames, filenames in os.walk(\"/home/jkok1g14/Documents/GRS1915+105/data/Std1_PCU2\"):    \n",
    "    for filename in fnmatch.filter(filenames, \"*_std1_lc.txt\"):\n",
    "        if filename in belloni_files:\n",
    "            lc_dirs.append(os.path.join(root, filename))\n",
    "\n",
    "            \n",
    "#make 2D arrays for light curves, with columns of counts and time values\n",
    "for lc in lc_dirs:\n",
    "    ids.append(lc.split(\"/\")[-1].split(\"_\")[0])\n",
    "    f=np.loadtxt(lc)\n",
    "    f=np.transpose(f)#,axis=1)\n",
    "    f=f[0:2]\n",
    "    ###1s average and time check to eliminate points outside of GTIs\n",
    "    f8t = np.mean(f[0][:(len(f[0])//8)*8].reshape(-1, 8), axis=1)\n",
    "    f8c = np.mean(f[1][:(len(f[1])//8)*8].reshape(-1, 8), axis=1)\n",
    "    rm_points = []\n",
    "    for i in range(len(f8t)-1):\n",
    "        delta = f8t[i+1]-f8t[i]\n",
    "        if delta > 1.0:\n",
    "            rm_points.append(i+1)\n",
    "            \n",
    "####### normalise the count rates! think about the effect of 0-1 normalisation on the distance calculation\n",
    "            \n",
    "    times=np.delete(f8t,rm_points)\n",
    "    counts=np.delete(f8c,rm_points)\n",
    "    lcs.append(np.stack((times,counts)))\n",
    "print(\"No. of light curves prepared: {}\".format(len(lcs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_belloni = open('1915Belloniclass_updated.dat')\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "\n",
    "ob_state = {}\n",
    "for state, obs in belloni_clean.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob+\"_std1_lc.txt\"] = state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alpha', 6.0),\n",
       " ('beta', 14.0),\n",
       " ('gamma', 10.0),\n",
       " ('delta', 21.0),\n",
       " ('theta', 17.0),\n",
       " ('kappa', 10.0),\n",
       " ('lambda', 2.0),\n",
       " ('mu', 5.0),\n",
       " ('nu', 1.0),\n",
       " ('rho', 17.0),\n",
       " ('phi', 7.0),\n",
       " ('chi', 75.0),\n",
       " ('eta', 1.0),\n",
       " ('omega', 1.0)]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states=[]\n",
    "for obID, state in ob_state.items():\n",
    "    if state not in states:\n",
    "        states.append(state)\n",
    "counts=np.zeros(len(states))\n",
    "for obID, state in ob_state.items():\n",
    "    if obID.split(\"_\")[0] in pool:\n",
    "        counts[np.where(np.array(states)==state)]+=1\n",
    "state_count=[]\n",
    "for i, state in enumerate(states):\n",
    "    state_count.append((state,counts[i]))\n",
    "state_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': ['20187-02-01-00',\n",
       "  '20187-02-01-01',\n",
       "  '20402-01-22-00',\n",
       "  '20402-01-23-00',\n",
       "  '20402-01-24-01',\n",
       "  '20402-01-27-00',\n",
       "  '20402-01-28-00',\n",
       "  '20402-01-30-02'],\n",
       " 'beta': ['10408-01-10-00',\n",
       "  '10408-01-21-00',\n",
       "  '20402-01-43-02',\n",
       "  '20402-01-44-00',\n",
       "  '20402-01-45-03',\n",
       "  '20402-01-46-00',\n",
       "  '20402-01-52-01',\n",
       "  '20402-01-52-02',\n",
       "  '20402-01-53-00',\n",
       "  '40703-01-18-00',\n",
       "  '40703-01-19-00',\n",
       "  '40703-01-19-01',\n",
       "  '40703-01-22-00',\n",
       "  '40703-01-22-01',\n",
       "  '40703-01-35-00',\n",
       "  '40703-01-35-01'],\n",
       " 'gamma': ['20402-01-37-00',\n",
       "  '20402-01-37-02',\n",
       "  '20402-01-38-00',\n",
       "  '20402-01-39-00',\n",
       "  '20402-01-39-02',\n",
       "  '20402-01-40-00',\n",
       "  '20402-01-55-00',\n",
       "  '20402-01-56-00',\n",
       "  '20402-01-57-00',\n",
       "  '40703-01-13-00',\n",
       "  '40115-01-07-00',\n",
       "  '40703-01-31-00'],\n",
       " 'delta': ['10408-01-14-00',\n",
       "  '10408-01-14-01',\n",
       "  '10408-01-14-02',\n",
       "  '10408-01-14-03',\n",
       "  '10408-01-14-04',\n",
       "  '10408-01-14-05',\n",
       "  '10408-01-14-07',\n",
       "  '10408-01-14-08',\n",
       "  '10408-01-14-09',\n",
       "  '10408-01-17-00',\n",
       "  '10408-01-17-03',\n",
       "  '10408-01-18-01',\n",
       "  '10408-01-18-04',\n",
       "  '20402-01-41-00',\n",
       "  '20402-01-41-01',\n",
       "  '20402-01-41-03',\n",
       "  '20402-01-42-00',\n",
       "  '20402-01-54-00',\n",
       "  '40703-01-33-01',\n",
       "  '40703-01-33-02',\n",
       "  '40703-01-34-00',\n",
       "  '40703-01-34-01',\n",
       "  '40703-01-34-02'],\n",
       " 'theta': ['10408-01-15-00',\n",
       "  '10408-01-15-01',\n",
       "  '10408-01-15-02',\n",
       "  '10408-01-15-03',\n",
       "  '10408-01-15-04',\n",
       "  '10408-01-15-05',\n",
       "  '10408-01-16-00',\n",
       "  '10408-01-16-01',\n",
       "  '10408-01-16-02',\n",
       "  '10408-01-16-03',\n",
       "  '10408-01-16-04',\n",
       "  '20402-01-45-02',\n",
       "  '20186-03-02-00',\n",
       "  '20186-03-02-01',\n",
       "  '20186-03-02-03',\n",
       "  '20186-03-02-04',\n",
       "  '40703-01-36-02',\n",
       "  '40703-01-38-00',\n",
       "  '40703-01-38-03'],\n",
       " 'kappa': ['20402-01-33-00',\n",
       "  '20402-01-35-00',\n",
       "  '40703-01-12-00',\n",
       "  '40703-01-14-00',\n",
       "  '40703-01-14-01',\n",
       "  '40703-01-14-02',\n",
       "  '40703-01-24-00',\n",
       "  '40703-01-25-00',\n",
       "  '40703-01-26-00',\n",
       "  '40703-01-30-00',\n",
       "  '40703-01-30-01',\n",
       "  '40703-01-30-02'],\n",
       " 'lambda': ['10258-01-10-00',\n",
       "  '20402-01-36-00',\n",
       "  '20402-01-36-01',\n",
       "  '20402-01-37-01'],\n",
       " 'mu': ['10408-01-08-00',\n",
       "  '10408-01-34-00',\n",
       "  '10408-01-35-00',\n",
       "  '10408-01-36-00',\n",
       "  '20402-01-43-01',\n",
       "  '20402-01-45-01',\n",
       "  '20402-01-53-01'],\n",
       " 'nu': ['10408-01-40-00', '10408-01-41-00', '10408-01-44-00'],\n",
       " 'rho': ['20402-01-03-00',\n",
       "  '20402-01-27-02',\n",
       "  '20402-01-30-00',\n",
       "  '20402-01-31-00',\n",
       "  '20402-01-31-01',\n",
       "  '20402-01-31-02',\n",
       "  '20402-01-32-00',\n",
       "  '20402-01-32-01',\n",
       "  '20402-01-34-00',\n",
       "  '20402-01-34-01',\n",
       "  '30703-01-27-00',\n",
       "  '30703-01-28-00',\n",
       "  '30703-01-28-01',\n",
       "  '30703-01-29-00',\n",
       "  '40703-01-07-00',\n",
       "  '40403-01-05-02',\n",
       "  '40703-01-23-00',\n",
       "  '40703-01-23-01',\n",
       "  '40703-01-23-02'],\n",
       " 'phi': ['10408-01-09-00',\n",
       "  '10408-01-11-00',\n",
       "  '10408-01-12-00',\n",
       "  '10408-01-17-01',\n",
       "  '10408-01-17-02',\n",
       "  '10408-01-19-00',\n",
       "  '10408-01-19-01',\n",
       "  '10408-01-19-02',\n",
       "  '10408-01-20-00',\n",
       "  '10408-01-20-01'],\n",
       " 'chi1': ['10408-01-22-00',\n",
       "  '10408-01-22-01',\n",
       "  '10408-01-23-00',\n",
       "  '10408-01-24-00',\n",
       "  '10408-01-25-00',\n",
       "  '10408-01-27-00',\n",
       "  '10408-01-28-00',\n",
       "  '10408-01-29-00',\n",
       "  '10408-01-30-00',\n",
       "  '10408-01-31-00',\n",
       "  '20186-03-02-05',\n",
       "  '20186-03-02-06',\n",
       "  '30402-01-11-00',\n",
       "  '30402-01-12-01',\n",
       "  '30402-01-12-02',\n",
       "  '30402-01-12-03',\n",
       "  '30703-01-31-00',\n",
       "  '30703-01-33-00',\n",
       "  '30703-01-35-00',\n",
       "  '30703-01-36-00',\n",
       "  '40703-01-15-00',\n",
       "  '40703-01-15-01',\n",
       "  '40703-01-15-02',\n",
       "  '40703-01-16-00',\n",
       "  '40703-01-16-01',\n",
       "  '40703-01-16-02',\n",
       "  '40703-01-17-00',\n",
       "  '40703-01-17-01',\n",
       "  '40703-01-20-00',\n",
       "  '40703-01-20-01',\n",
       "  '40703-01-20-02',\n",
       "  '40703-01-20-03',\n",
       "  '40703-01-21-01',\n",
       "  '40703-01-38-01',\n",
       "  '40703-01-38-02',\n",
       "  '40703-01-41-00',\n",
       "  '40703-01-41-01',\n",
       "  '40703-01-41-02',\n",
       "  '40703-01-41-03',\n",
       "  '40703-01-42-01',\n",
       "  '40703-01-42-03'],\n",
       " 'chi2': ['20402-01-04-00',\n",
       "  '20402-01-05-00',\n",
       "  '20402-01-07-00',\n",
       "  '20402-01-08-00',\n",
       "  '20402-01-08-01',\n",
       "  '20402-01-09-00',\n",
       "  '20402-01-10-00',\n",
       "  '20402-01-11-00',\n",
       "  '20402-01-12-00',\n",
       "  '20402-01-13-00',\n",
       "  '20402-01-14-00',\n",
       "  '20402-01-15-00',\n",
       "  '20402-01-16-00',\n",
       "  '20402-01-17-00',\n",
       "  '20402-01-18-00',\n",
       "  '20402-01-19-00',\n",
       "  '20402-01-19-00',\n",
       "  '20402-01-20-00',\n",
       "  '20402-01-21-01'],\n",
       " 'chi3': ['20402-01-49-00',\n",
       "  '20402-01-49-01',\n",
       "  '20402-01-50-00',\n",
       "  '20402-01-51-00',\n",
       "  '20402-01-52-00'],\n",
       " 'chi4': ['10408-01-31-00',\n",
       "  '10408-01-32-00',\n",
       "  '10408-01-33-00',\n",
       "  '10408-01-42-00',\n",
       "  '10408-01-43-00',\n",
       "  '10408-01-45-00',\n",
       "  '20402-01-24-00',\n",
       "  '20402-01-25-00',\n",
       "  '20402-01-26-01',\n",
       "  '20402-01-26-02',\n",
       "  '20402-01-27-01',\n",
       "  '20402-01-27-03',\n",
       "  '20402-01-29-00',\n",
       "  '20402-01-48-00'],\n",
       " 'eta': ['80127-01-01-00', '80127-01-02-01', '80127-01-02-00'],\n",
       " 'omega': ['40703-01-13-01', '40703-01-27-00', '40703-01-29-01']}"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "belloni_clean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
