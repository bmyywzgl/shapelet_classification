{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import shapelets as sha\n",
    "importlib.reload(sha)\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the observation ids of classifications from Huppenkothen et al. and create an ob_state dictionary of (presumed) file names, which contain data of those observations, against the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_state=sha.import_labels('1915Belloniclass_updated.dat', \"_std1_lc.txt\")\n",
    "clean_belloni = open('1915Belloniclass_updated.dat')\n",
    "lines = clean_belloni.readlines()\n",
    "states = lines[0].split()\n",
    "belloni_clean = {}\n",
    "for h,l in zip(states, lines[1:]):\n",
    "    belloni_clean[h] = l.split()\n",
    "    #state: obsID1, obsID2...\n",
    "ob_state = {}\n",
    "for state, obs in belloni_clean.items():\n",
    "    if state == \"chi1\" or state == \"chi2\" or state == \"chi3\" or state == \"chi4\": state = \"chi\"\n",
    "    for ob in obs:\n",
    "        ob_state[ob] = state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of classifications for every class.\n",
    "Only classes with an acceptable number of samples should be analysed. Let's make the acceptable number at least 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alpha', 8.0),\n",
       " ('beta', 16.0),\n",
       " ('gamma', 12.0),\n",
       " ('delta', 23.0),\n",
       " ('theta', 19.0),\n",
       " ('kappa', 12.0),\n",
       " ('lambda', 4.0),\n",
       " ('mu', 7.0),\n",
       " ('nu', 3.0),\n",
       " ('rho', 19.0),\n",
       " ('phi', 9.0),\n",
       " ('chi', 77.0),\n",
       " ('eta', 3.0),\n",
       " ('omega', 3.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states=[]\n",
    "for obID, state in ob_state.items():\n",
    "    if state not in states:\n",
    "        states.append(state)\n",
    "counts=np.zeros(len(states))\n",
    "for obID, state in ob_state.items():\n",
    "    if obID in pool:\n",
    "        counts[np.where(np.array(states)==state)]+=1\n",
    "state_count=[]\n",
    "for i, state in enumerate(states):\n",
    "    state_count.append((state,counts[i]))\n",
    "state_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the availability of the files from ob_state and append their names to the pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available = []\n",
    "pool=[]\n",
    "for root, dirnames, filenames in os.walk(\"/home/jkok1g14/Documents/GRS1915+105/data/Std1_PCU2\"):\n",
    "    for filename in fnmatch.filter(filenames, \"*_std1_lc.txt\"):\n",
    "        available.append(filename)\n",
    "for ob, state in ob_state.items():\n",
    "    if ob+\"_std1_lc.txt\" in available:\n",
    "        pool.append(ob)  \n",
    "len(pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create arrays of light curve time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of arrays with time and counts for the set of Belloni classified observations\n",
    "lc_dirs=[]\n",
    "lcs=[]\n",
    "ids=[]\n",
    "for root, dirnames, filenames in os.walk(\"/home/jkok1g14/Documents/GRS1915+105/data/Std1_PCU2\"):    \n",
    "    for filename in fnmatch.filter(filenames, \"*_std1_lc.txt\"):\n",
    "        if filename.split(\"_\")[0] in pool:\n",
    "            lc_dirs.append(os.path.join(root, filename))\n",
    "\n",
    "            \n",
    "#make 2D arrays for light curves, with columns of counts and time values\n",
    "for lc in lc_dirs:\n",
    "    ids.append(lc.split(\"/\")[-1].split(\"_\")[0])\n",
    "    f=np.loadtxt(lc)\n",
    "    f=np.transpose(f)#,axis=1)\n",
    "    f=f[0:2]\n",
    "    ###1s average and time check to eliminate points outside of GTIs\n",
    "    f8t = np.mean(f[0][:(len(f[0])//8)*8].reshape(-1, 8), axis=1)\n",
    "    f8c = np.mean(f[1][:(len(f[1])//8)*8].reshape(-1, 8), axis=1)\n",
    "    f8c=f8c/np.max(f8c)\n",
    "    rm_points = []\n",
    "    skip=False\n",
    "    for i in range(len(f8t)-1):\n",
    "        if skip==True:\n",
    "            skip=False\n",
    "            continue\n",
    "        delta = f8t[i+1]-f8t[i]\n",
    "        if delta > 1.0:\n",
    "            rm_points.append(i+1)\n",
    "            skip=True\n",
    "            \n",
    "####### normalise the count rates! think about the effect of 0-1 normalisation on the distance calculation\n",
    "            \n",
    "    times=np.delete(f8t,rm_points)\n",
    "    counts=np.delete(f8c,rm_points)\n",
    "    lcs.append(np.stack((times,counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an ordered list of classifications from the order ids list and the ob_state dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_classes=[]\n",
    "for i in ids:\n",
    "    lc_classes.append(ob_state[i])\n",
    "lc_classes\n",
    "\n",
    "drop_classes=[]\n",
    "for clas, no in Counter(lc_classes).items():\n",
    "    if no<7:\n",
    "        drop_classes.append(clas)\n",
    "\n",
    "lcs_abu = []\n",
    "classes_abu = []\n",
    "ids_abu = []\n",
    "for n, lc in enumerate(lc_classes):\n",
    "    if lc not in drop_classes:\n",
    "        classes_abu.append(lc)\n",
    "        lcs_abu.append(lcs[n])\n",
    "        ids_abu.append(ids[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, id_train, id_test = train_test_split(lcs_abu, classes_abu, ids_abu, test_size=0.5, random_state=0, stratify=classes_abu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
